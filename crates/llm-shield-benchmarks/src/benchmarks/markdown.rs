//! Markdown report generation for benchmark results
//!
//! This module provides utilities for generating human-readable
//! markdown summaries of benchmark results.

use super::{BenchmarkResult, OutputPaths};
use chrono::Utc;
use std::fs::File;
use std::io::{BufWriter, Write};

/// Generate a markdown summary of benchmark results
///
/// Creates a formatted markdown document with:
/// - Header and timestamp
/// - Summary statistics
/// - Individual result details
/// - Performance comparison table
///
/// # Arguments
///
/// * `results` - Vector of BenchmarkResults to summarize
///
/// # Returns
///
/// Markdown string containing the summary
pub fn generate_summary_markdown(results: &[BenchmarkResult]) -> String {
    let mut md = String::new();

    // Header
    md.push_str("# LLM Shield Benchmark Results\n\n");
    md.push_str(&format!(
        "Generated: {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    // Summary statistics
    let total = results.len();
    let successful = results
        .iter()
        .filter(|r| !r.metrics.get("error").is_some())
        .count();
    let failed = total - successful;

    md.push_str("## Summary\n\n");
    md.push_str(&format!("- **Total Benchmarks**: {}\n", total));
    md.push_str(&format!("- **Successful**: {}\n", successful));
    md.push_str(&format!("- **Failed**: {}\n", failed));
    md.push_str(&format!(
        "- **Success Rate**: {:.1}%\n\n",
        (successful as f64 / total as f64) * 100.0
    ));

    // Results table
    md.push_str("## Results\n\n");
    md.push_str("| Target ID | Status | Key Metrics |\n");
    md.push_str("|-----------|--------|-------------|\n");

    for result in results {
        let status = if result.metrics.get("error").is_some() {
            "âŒ Failed"
        } else {
            "âœ… Passed"
        };

        let metrics_summary = summarize_metrics(&result.metrics);
        md.push_str(&format!(
            "| {} | {} | {} |\n",
            result.target_id, status, metrics_summary
        ));
    }

    md.push_str("\n");

    // Detailed results
    md.push_str("## Detailed Results\n\n");

    for result in results {
        md.push_str(&format!("### {}\n\n", result.target_id));
        md.push_str(&format!(
            "**Timestamp**: {}\n\n",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));

        if let Some(error) = result.metrics.get("error") {
            md.push_str(&format!("**Error**: {}\n\n", error));
        } else {
            md.push_str("**Metrics**:\n\n");
            md.push_str("```json\n");
            md.push_str(&serde_json::to_string_pretty(&result.metrics).unwrap_or_default());
            md.push_str("\n```\n\n");
        }
    }

    // Footer
    md.push_str("---\n\n");
    md.push_str("*This report was generated by the LLM Shield canonical benchmark interface.*\n");
    md.push_str("*Compatible with all 25 benchmark-target repositories.*\n");

    md
}

/// Summarize metrics for table display
fn summarize_metrics(metrics: &serde_json::Value) -> String {
    let mut parts = Vec::new();

    // Look for common metric keys
    let keys = [
        ("mean_ms", "Mean"),
        ("p50_ms", "P50"),
        ("p95_ms", "P95"),
        ("p99_ms", "P99"),
        ("latency_ms", "Latency"),
        ("throughput_rps", "Throughput"),
        ("memory_mb", "Memory"),
        ("execution_time_ms", "Exec Time"),
    ];

    for (key, label) in keys {
        if let Some(value) = metrics.get(key) {
            if let Some(num) = value.as_f64() {
                parts.push(format!("{}: {:.2}", label, num));
            }
        }
    }

    if parts.is_empty() {
        if let Some(error) = metrics.get("error") {
            return format!("Error: {}", error);
        }
        return "No metrics".to_string();
    }

    parts.join(", ")
}

/// Write summary markdown to the canonical location
///
/// Writes to benchmarks/output/summary.md
///
/// # Arguments
///
/// * `results` - Vector of BenchmarkResults to summarize
/// * `paths` - OutputPaths configuration
pub fn write_summary_markdown(
    results: &[BenchmarkResult],
    paths: &OutputPaths,
) -> Result<(), std::io::Error> {
    let markdown = generate_summary_markdown(results);

    let file = File::create(&paths.summary)?;
    let mut writer = BufWriter::new(file);
    writer.write_all(markdown.as_bytes())?;

    tracing::info!("Wrote summary to {:?}", paths.summary);

    Ok(())
}

/// Generate a comparison table between two result sets
///
/// Useful for comparing performance across versions or implementations
pub fn generate_comparison_markdown(
    baseline: &[BenchmarkResult],
    current: &[BenchmarkResult],
    baseline_label: &str,
    current_label: &str,
) -> String {
    let mut md = String::new();

    md.push_str("# Benchmark Comparison\n\n");
    md.push_str(&format!(
        "Comparing **{}** vs **{}**\n\n",
        baseline_label, current_label
    ));
    md.push_str(&format!(
        "Generated: {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    md.push_str("| Target | {} | {} | Change |\n");
    md.push_str("|--------|-----|-----|--------|\n");

    // Create lookup for baseline results
    let baseline_map: std::collections::HashMap<_, _> = baseline
        .iter()
        .map(|r| (r.target_id.clone(), r))
        .collect();

    for current_result in current {
        if let Some(baseline_result) = baseline_map.get(&current_result.target_id) {
            // Compare execution times if available
            let baseline_time = baseline_result.get_metric_f64("execution_time_ms");
            let current_time = current_result.get_metric_f64("execution_time_ms");

            let change = match (baseline_time, current_time) {
                (Some(b), Some(c)) => {
                    let pct = ((c - b) / b) * 100.0;
                    if pct < 0.0 {
                        format!("ðŸŸ¢ {:.1}% faster", -pct)
                    } else if pct > 0.0 {
                        format!("ðŸ”´ {:.1}% slower", pct)
                    } else {
                        "âž¡ï¸ No change".to_string()
                    }
                }
                _ => "N/A".to_string(),
            };

            md.push_str(&format!(
                "| {} | {:.2}ms | {:.2}ms | {} |\n",
                current_result.target_id,
                baseline_time.unwrap_or(0.0),
                current_time.unwrap_or(0.0),
                change
            ));
        }
    }

    md
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    #[test]
    fn test_generate_summary_markdown() {
        let results = vec![
            BenchmarkResult::new(
                "test-latency",
                json!({
                    "mean_ms": 0.5,
                    "p95_ms": 1.0
                }),
            ),
            BenchmarkResult::new(
                "test-throughput",
                json!({
                    "throughput_rps": 10000
                }),
            ),
        ];

        let markdown = generate_summary_markdown(&results);

        assert!(markdown.contains("# LLM Shield Benchmark Results"));
        assert!(markdown.contains("test-latency"));
        assert!(markdown.contains("test-throughput"));
        assert!(markdown.contains("âœ… Passed"));
    }

    #[test]
    fn test_summarize_metrics() {
        let metrics = json!({
            "mean_ms": 0.5,
            "p95_ms": 1.0,
            "throughput_rps": 10000
        });

        let summary = summarize_metrics(&metrics);

        assert!(summary.contains("Mean: 0.50"));
        assert!(summary.contains("P95: 1.00"));
        assert!(summary.contains("Throughput: 10000.00"));
    }
}
